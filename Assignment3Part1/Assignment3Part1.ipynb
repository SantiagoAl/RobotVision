{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3O2VlN0h3cv"
      },
      "source": [
        "# Import the nessecary libraries\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMJ-HT71iGDY"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  # The initializer or Constructor\n",
        "  def __init__(self, mode):\n",
        "    super(ConvNet, self).__init__()\n",
        "\n",
        "    # Define various layers here\n",
        "    self.fc1 = nn.Linear(28*28, 18)\n",
        "    self.fc2 = nn.Linear(18, 15)\n",
        "    self.fc3 = nn.Linear(15, 10)\n",
        "\n",
        "    self.deepfc1 = nn.Linear(28*28, 250)\n",
        "    self.deepfc2 = nn.Linear(250, 250)\n",
        "    self.deepfc3 = nn.Linear(250, 250)\n",
        "    self.deepfc4 = nn.Linear(250, 230)\n",
        "    self.deepfc5 = nn.Linear(230, 230)\n",
        "    self.deepfc6 = nn.Linear(230, 10)\n",
        "\n",
        "    # This will select the forward pass function based on mode for the ConvNet.\n",
        "    # During the creation of each ConvNet model, you will assign one of the valid modes.\n",
        "    # This will fix the forward function (and the network graph) for the entire\n",
        "    # training/testing\n",
        "    if mode == 1:\n",
        "      self.forward = self.model_1\n",
        "    elif mode == 2:\n",
        "      self.forward = self.model_2\n",
        "    elif mode == 3:\n",
        "      self.forward = self.model_3\n",
        "    else:\n",
        "      print('Invalid mode {} selected. Select between 1-3'.format(mode))\n",
        "      exit(0)\n",
        "\n",
        "  # Baseline sample model\n",
        "  def model_0(self, X):\n",
        "    # ======================================================================\n",
        "    # Three fully connected layers with activation\n",
        "\n",
        "    X = torch.flatten(X, start_dim=1)\n",
        "    X = self.fc1(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.fc2(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.fc3(X)\n",
        "    X = torch.sigmoid(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "  # Base line model. Task 1\n",
        "  def model_1(self, X):\n",
        "    # ======================================================================\n",
        "    # Three fully connected layers without activation\n",
        "\n",
        "    X = torch.flatten(X, start_dim=1)\n",
        "    X = self.fc1(X)\n",
        "    X = self.fc2(X)\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "  # Task 2\n",
        "  def model_2(self, X):\n",
        "    # ======================================================================\n",
        "    # Train with activation (use model 1 from task 1)\n",
        "\n",
        "    X = torch.flatten(X, start_dim=1)\n",
        "    X = self.fc1(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.fc2(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.fc3(X)\n",
        "    X = torch.sigmoid(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "  # Task 3\n",
        "  def model_3(self, X):\n",
        "    # ======================================================================\n",
        "    # Change number of fully connected layers and number neurons from model\n",
        "    # 2 in task 2\n",
        "\n",
        "    X = torch.flatten(X, start_dim=1)\n",
        "    X = F.relu(self.deepfc1(X))\n",
        "    X = F.relu(self.deepfc2(X))\n",
        "    X = F.relu(self.deepfc3(X))\n",
        "    X = F.relu(self.deepfc4(X))\n",
        "    X = F.relu(self.deepfc5(X))\n",
        "    X = F.relu(self.deepfc6(X))\n",
        "    X = torch.sigmoid(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DqHDGHIpetr"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n",
        "  '''\n",
        "  Trains the model for an epoch and optimizes it.\n",
        "  model: The model to train. Should already be in correct device.\n",
        "  device: 'cuda' or 'cpu'.\n",
        "  train_loader: dataloader for training samples.\n",
        "  optimizer: optimizer to use for model parameter updates.\n",
        "  criterion: used to compute loss for prediction and target \n",
        "  epoch: Current epoch to train for.\n",
        "  batch_size: Batch size to be used.\n",
        "  '''\n",
        "  \n",
        "  # Set model to train mode before each epoch\n",
        "  model.train()\n",
        "\n",
        "  # Empty list to store losses\n",
        "  losses = []\n",
        "  correct = 0\n",
        "  \n",
        "  # Iterate over entire training samples (1 epoch)\n",
        "  for batch_idx, batch_sample in enumerate(train_loader):\n",
        "    data, target = batch_sample\n",
        "\n",
        "    # Push data/label to correct device\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Do forward pass for current set of data\n",
        "    output = model(data)\n",
        "    \n",
        "    # ======================================================================\n",
        "    # Compute loss based on criterion\n",
        "    # ----------------- YOUR CODE HERE ----------------------\n",
        "    #\n",
        "    # Remove NotImplementedError and assign correct loss function.\n",
        "    # loss = NotImplementedError()\n",
        "    loss = criterion(output, target)\n",
        "    \n",
        "    # Computes gradient based on final loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Store loss\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Optimize model parameters based on learning rate and gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get predicted index by selecting maximum log-probability\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # ======================================================================\n",
        "    # Count correct predictions overall\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  \n",
        "  train_loss = float(np.mean(losses))\n",
        "  train_acc = correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "  print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
        "        100. * correct / ((batch_idx+1) * batch_size)))\n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdG3cOA8yxl3"
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "  '''\n",
        "  Tests the model.\n",
        "  model: The model to train. Should already be in correct device.\n",
        "  device: 'cuda' or 'cpu'.\n",
        "  test_loader: dataloader for test samples.\n",
        "  '''\n",
        "\n",
        "  # Set model to eval mode to notify all layers\n",
        "  model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct = 0\n",
        "\n",
        "  # Set torch.no_grad() to disable gradient computation and backpropagation\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, sample in enumerate(test_loader):\n",
        "      data, target = sample\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # Predict for data by doing forward pass\n",
        "      output = model(data)\n",
        "\n",
        "      # ======================================================================\n",
        "      # Compute loss based on same criterion as training\n",
        "      loss = F.cross_entropy(output, target, reduction='mean')\n",
        "\n",
        "      # Append loss to overall test loss\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      # Get predicted index by selecting maximum log-probability\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "      # ======================================================================\n",
        "      # Count correct predictions overall\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss = float(np.mean(losses))\n",
        "  accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "  print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "\n",
        "  return test_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_FYIlPi7OJU"
      },
      "source": [
        "def run_main(FLAGS):\n",
        "  # Check if cuda is available\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  \n",
        "  # Set proper devide based on cuda availability\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  print(\"Torch device selected: \", device)\n",
        "\n",
        "  #Initialize the model and send to device\n",
        "  model = ConvNet(FLAGS.mode).to(device)\n",
        "  \n",
        "  # Initialize the criterion for loss computation \n",
        "  # ======================================================================\n",
        "  # Remove NotImplementedError and assign correct loss function.\n",
        "  # criterion = NotImplementedError()\n",
        "  criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  # Initialize optimizer type\n",
        "  optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n",
        "\n",
        "  # Create transformations to apply to each data sample\n",
        "  # Can specify variations such as image flip, color flip, random crop, ...\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307,), (0.3081,))\n",
        "  ])\n",
        "\n",
        "  # Load datasets for training and testing\n",
        "  # Inbuild datasets available in torchvision (check documentation online)\n",
        "  dataset1 = datasets.MNIST('./data/', train=True, download=True, transform=transform)\n",
        "  dataset2 = datasets.MNIST('./data/', train=False, transform=transform)\n",
        "  train_loader = DataLoader(dataset1, batch_size=FLAGS.batch_size,\n",
        "                            shuffle=True, num_workers=4)\n",
        "  test_loader = DataLoader(dataset2, batch_size=FLAGS.batch_size,\n",
        "                           shuffle=False, num_workers=4)\n",
        "  \n",
        "  best_accuracy = 0.0\n",
        "\n",
        "  # Run training for n_epochs specified in config\n",
        "  for epoch in range(1, FLAGS.num_epochs + 1):\n",
        "    print(\"\\nEpoch: \", epoch)\n",
        "    train_loss, train_accuracy = train(model, device, train_loader,\n",
        "                                       optimizer, criterion, epoch, FLAGS.batch_size)\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "    if test_accuracy > best_accuracy:\n",
        "      best_accuracy = test_accuracy\n",
        "\n",
        "  print('\\naccuracy is {:2.2f}'.format(best_accuracy))\n",
        "\n",
        "  print('Training and evaluation finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpWiKWwoPIUH",
        "outputId": "9883c1b1-86be-471b-ae0e-4deb16300230"
      },
      "source": [
        "# Set parameters for Sparse Autoencoder\n",
        "parser = argparse.ArgumentParser('CNN Exercise.')\n",
        "parser.add_argument('--mode', type=int, default=1,\n",
        "                    help='Select mode between 1-3')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.01,\n",
        "                    help='Initial learning rate')\n",
        "parser.add_argument('--num_epochs', type=int, default=20,\n",
        "                    help='Number of epochs to run trainer')\n",
        "parser.add_argument('--batch_size', type=int, default=10,\n",
        "                    help='Batch size. Must divide evenly into the dataset sizes.')\n",
        "parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                    help='Directory to put logging')\n",
        "\n",
        "FLAGS = None\n",
        "FLAGS, unparsed = parser.parse_known_args()\n",
        "\n",
        "print(\"Mode: {}\".format(FLAGS.mode))\n",
        "print(\"LR: {}\".format(FLAGS.learning_rate))\n",
        "print(\"Batch size: {}\".format(FLAGS.batch_size))\n",
        "\n",
        "run_main(FLAGS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mode: 1\n",
            "LR: 0.01\n",
            "Batch size: 10\n",
            "Torch device selected:  cuda\n",
            "\n",
            "Epoch:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train set: Average loss: 1.7157, Accuracy: 45425/60000 (76%)\n",
            "Test set: Average loss: 1.6060, Accuracy: 8146/10000 (81%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.6003, Accuracy: 48795/60000 (81%)\n",
            "Test set: Average loss: 1.5863, Accuracy: 8214/10000 (82%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.5859, Accuracy: 49744/60000 (83%)\n",
            "Test set: Average loss: 1.5762, Accuracy: 8372/10000 (84%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.5777, Accuracy: 50830/60000 (85%)\n",
            "Test set: Average loss: 1.5722, Accuracy: 8569/10000 (86%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 1.5728, Accuracy: 51620/60000 (86%)\n",
            "Test set: Average loss: 1.5669, Accuracy: 8836/10000 (88%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 1.5690, Accuracy: 52395/60000 (87%)\n",
            "Test set: Average loss: 1.5644, Accuracy: 8744/10000 (87%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 1.5649, Accuracy: 53261/60000 (89%)\n",
            "Test set: Average loss: 1.5609, Accuracy: 8984/10000 (90%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 1.5605, Accuracy: 53873/60000 (90%)\n",
            "Test set: Average loss: 1.5565, Accuracy: 9054/10000 (91%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 1.5579, Accuracy: 54016/60000 (90%)\n",
            "Test set: Average loss: 1.5560, Accuracy: 9086/10000 (91%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 1.5554, Accuracy: 54133/60000 (90%)\n",
            "Test set: Average loss: 1.5539, Accuracy: 9089/10000 (91%)\n",
            "\n",
            "Epoch:  11\n",
            "Train set: Average loss: 1.5534, Accuracy: 54274/60000 (90%)\n",
            "Test set: Average loss: 1.5523, Accuracy: 9082/10000 (91%)\n",
            "\n",
            "Epoch:  12\n",
            "Train set: Average loss: 1.5519, Accuracy: 54313/60000 (91%)\n",
            "Test set: Average loss: 1.5533, Accuracy: 9098/10000 (91%)\n",
            "\n",
            "Epoch:  13\n",
            "Train set: Average loss: 1.5506, Accuracy: 54401/60000 (91%)\n",
            "Test set: Average loss: 1.5503, Accuracy: 9108/10000 (91%)\n",
            "\n",
            "Epoch:  14\n",
            "Train set: Average loss: 1.5492, Accuracy: 54432/60000 (91%)\n",
            "Test set: Average loss: 1.5495, Accuracy: 9109/10000 (91%)\n",
            "\n",
            "Epoch:  15\n",
            "Train set: Average loss: 1.5480, Accuracy: 54563/60000 (91%)\n",
            "Test set: Average loss: 1.5506, Accuracy: 9106/10000 (91%)\n",
            "\n",
            "Epoch:  16\n",
            "Train set: Average loss: 1.5473, Accuracy: 54545/60000 (91%)\n",
            "Test set: Average loss: 1.5485, Accuracy: 9098/10000 (91%)\n",
            "\n",
            "Epoch:  17\n",
            "Train set: Average loss: 1.5464, Accuracy: 54519/60000 (91%)\n",
            "Test set: Average loss: 1.5496, Accuracy: 9083/10000 (91%)\n",
            "\n",
            "Epoch:  18\n",
            "Train set: Average loss: 1.5457, Accuracy: 54559/60000 (91%)\n",
            "Test set: Average loss: 1.5472, Accuracy: 9131/10000 (91%)\n",
            "\n",
            "Epoch:  19\n",
            "Train set: Average loss: 1.5450, Accuracy: 54631/60000 (91%)\n",
            "Test set: Average loss: 1.5476, Accuracy: 9079/10000 (91%)\n",
            "\n",
            "Epoch:  20\n",
            "Train set: Average loss: 1.5442, Accuracy: 54660/60000 (91%)\n",
            "Test set: Average loss: 1.5471, Accuracy: 9095/10000 (91%)\n",
            "\n",
            "accuracy is 91.31\n",
            "Training and evaluation finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}